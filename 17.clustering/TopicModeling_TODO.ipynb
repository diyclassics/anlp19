{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll explore topic modeling to discover broad themes in a collection of movie summaries.  To get started, install pyLDAvis and update numpy and gensim:\n",
    "\n",
    "```sh\n",
    "pip install pyLDAvis==2.1.2\n",
    "pip install numpy --upgrade\n",
    "pip install gensim --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob, os, re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords(filename):\n",
    "    stopwords={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            stopwords[line.rstrip()]=1\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're running topic modeling on texts with lots of names, we'll add the Jockers list of stopwords (which includes character names) to our stoplist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {k:1 for k in stopwords.words('english')}\n",
    "stop_words.update(read_stopwords(\"../data/jockers.stopwords\"))\n",
    "stop_words[\"'s\"]=1\n",
    "stop_words=list(stop_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(word, stopwords):\n",
    "    \n",
    "    \"\"\" Function to exclude words from a text \"\"\"\n",
    "    \n",
    "    # no stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    \n",
    "    # has to contain at least one letter\n",
    "    if re.search(\"[A-Za-z]\", word) is not None:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(inputDir, stopwords):\n",
    "    \"\"\" Read in movie documents (all ending in .txt) from an input folder\"\"\"\n",
    "    \n",
    "    docs=[]\n",
    "    names=[]\n",
    "    for idx, filename in enumerate(glob.glob(os.path.join(inputDir, '*.txt'))):\n",
    "        if idx >= 100:\n",
    "            break\n",
    "        with open(filename) as file:\n",
    "            tokens=nltk.word_tokenize(file.read().lower())\n",
    "            tokens=[x for x in tokens if filter(x, stopwords)]\n",
    "            docs.append(tokens)\n",
    "            basename=os.path.basename(filename)\n",
    "            name, file_extension = os.path.splitext(basename)\n",
    "            names.append(name)\n",
    "    return docs, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir=\"../data/movie_summaries\"\n",
    "data, doc_names=read_docs(text_dir, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the movie summaries into a Bag-of-Words representation using gensim's [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents \n",
    "# and no more than 50% of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(data)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try using gensim's built-in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of what the topics are by printing the top 10 words with highest $P(word \\mid topic)$ for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(' '.join([term for term, freq in lda_model.show_topic(i, topn=10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mallet is a great java package that tends to yield better results (in terms of topic coherence).  Download it from [here](http://mallet.cs.umass.edu/download.php) and point the `mallet_path` line below to its location on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path=\"/Users/mashabelyi/Downloads/mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "lda_mallet_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(' '.join([term for term, freq in lda_mallet_model.show_topic(i, topn=10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of understanding topics is to print out the documents that have the highest topic representation -- i.e., for a given topic $k$, the documents with highest $P(topic=k | document)$.  How much do the documents listed here align with your understanding of the topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model=lda_mallet_model \n",
    "\n",
    "topic_docs=[]\n",
    "for i in range(num_topics):\n",
    "    topic_docs.append({})\n",
    "for doc_id in range(len(corpus)):\n",
    "    doc_topics=topic_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_num, topic_prob in doc_topics:\n",
    "        topic_docs[topic_num][doc_id]=topic_prob\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print(\"%s\\n\" % ' '.join([term for term, freq in topic_model.show_topic(i, topn=10)]))\n",
    "    sorted_x = sorted(topic_docs[i].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_x[:5]:\n",
    "        print(\"%s\\t%.3f\\t%s\" % (i,v,doc_names[k]))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore topics using pyLDAvis, a visualization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(topic_model, corpus, dictionary, lambda_step=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Adapt the code above to operate on the documents you used for classification earlier in this course.    Execute the following for your trained model `lda_mallet_model` and `num_topics` so we can explore the topics that did emerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_topics):\n",
    "    print(' '.join([term for term, freq in lda_mallet_model.show_topic(i, topn=10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
